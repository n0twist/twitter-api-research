{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import config\n",
    "from matplotlib import pylab as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "color_palette = sns.color_palette(palette='muted', n_colors=None, desat=.75)\n",
    "sns.set(context='notebook', palette=color_palette, style='whitegrid', font='sans-serif', font_scale=1.5, color_codes=False, rc=None)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to the PostgreSQL database...\n",
      "PostgreSQL database version:\n",
      "('PostgreSQL 10.3, compiled by Visual C++ build 1800, 64-bit',)\n"
     ]
    }
   ],
   "source": [
    "conn = None\n",
    "try:\n",
    "    # read connection parameters\n",
    "    params = config.cfgAzureF17_local()\n",
    "    \n",
    "    paramsS03 = config.cfgAzureF03_local()\n",
    "\n",
    "    # connect to the PostgreSQL server\n",
    "    print('Connecting to the PostgreSQL database...')\n",
    "    conn = psycopg2.connect(**params)\n",
    "    \n",
    "    connS03 = psycopg2.connect(**paramsS03)\n",
    "\n",
    "    # create a cursor\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # execute a statement\n",
    "    print('PostgreSQL database version:')\n",
    "    cur.execute('SELECT version()')\n",
    "\n",
    "    # display the PostgreSQL database server version\n",
    "    db_version = cur.fetchone()\n",
    "    print(db_version)\n",
    "\n",
    "    # close the communication with the PostgreSQL\n",
    "    cur.close()\n",
    "\n",
    "except (Exception, psycopg2.DatabaseError) as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-cd13cf45944a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_sql_query\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SELECT * FROM tweets_info;\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'created_at'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'created_at'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'created_at'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtz_localize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"UTC\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtz_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Europe/Berlin\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Number of Tweets: %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtweets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\local\\AmlWorkbench\\Python\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mread_sql_query\u001b[1;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize)\u001b[0m\n\u001b[0;32m    329\u001b[0m     return pandas_sql.read_query(\n\u001b[0;32m    330\u001b[0m         \u001b[0msql\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoerce_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 331\u001b[1;33m         parse_dates=parse_dates, chunksize=chunksize)\n\u001b[0m\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\local\\AmlWorkbench\\Python\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mread_query\u001b[1;34m(self, sql, index_col, coerce_float, params, parse_dates, chunksize)\u001b[0m\n\u001b[0;32m   1443\u001b[0m                                         parse_dates=parse_dates)\n\u001b[0;32m   1444\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1445\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetchall_as_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcursor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1446\u001b[0m             \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\local\\AmlWorkbench\\Python\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36m_fetchall_as_list\u001b[1;34m(self, cur)\u001b[0m\n\u001b[0;32m   1452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fetchall_as_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1454\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1455\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1456\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tweets = pd.read_sql_query(\"SELECT * FROM tweets_info;\", conn, parse_dates=['created_at'] )\n",
    "tweets['created_at'] = tweets['created_at'].dt.tz_localize(\"UTC\").dt.tz_convert(\"Europe/Berlin\")\n",
    "\n",
    "print(\"Number of Tweets: %s\" %len(tweets))\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets[tweets['extracted'] == False ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsS03 = pd.read_sql_query(\"SELECT * FROM tweets_info;\", connS03, parse_dates=['created_at'] )\n",
    "tweetsS03['created_at'] = tweetsS03['created_at'].dt.tz_localize(\"UTC\").dt.tz_convert(\"Europe/Berlin\")\n",
    "\n",
    "print(\"Number of Tweets: %s\" %len(tweetsS03))\n",
    "tweetsS03.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweetsS03[tweetsS03['extracted'] == False ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dates_values = tweets['created_at'][(tweets['created_at'].dt.day == 23) & (tweets['extracted'] == False) & (tweets['created_at'].dt.year == 2018) & ((tweets['created_at'].dt.week == 17) | (tweets['created_at'].dt.week == 16))].value_counts()\n",
    "#dates_values = tweets['created_at'][(tweets['created_at'].dt.year == 2018) & (tweets['created_at'].dt.week == 7 )].value_counts()\n",
    "week_s03 = tweetsS03['created_at'][(tweetsS03['extracted'] == False)].value_counts()\n",
    "week_s17 = tweets['created_at'][(tweets['extracted'] == False)].value_counts()\n",
    "\n",
    "print(week_s03.resample(\"D\").sum().values)\n",
    "print(week_s17.resample(\"D\").sum().values)\n",
    "plt.figure(figsize=(16,6))\n",
    "#fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "\n",
    "concat_values = np.concatenate([week_s03.resample(\"D\").sum().values, week_s17.resample(\"D\").sum().values])\n",
    "concat_types = 7 * [\"cw 03\"] + 7 * [\"cw 17\"]\n",
    "concat_days = 2 * ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n",
    "print(concat_values)\n",
    "print(concat_types)\n",
    "print(concat_days)\n",
    "\n",
    "data = {'day': concat_days,'number of tweets':concat_values, 'calendar week': concat_types}\n",
    "df_days_stats = pd.DataFrame(data=data)\n",
    "\n",
    "df_days_stats\n",
    "ax = sns.barplot(x=\"day\", y=\"number of tweets\", hue=\"calendar week\", data=df_days_stats)\n",
    "ax.set_title(\"Tweets collected via Filtered Stream\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "plt.savefig(\"streaming_stats/filterd-count-by-day.png\", transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dates_values = tweets['created_at'][(tweets['created_at'].dt.day == 23) & (tweets['extracted'] == False) & (tweets['created_at'].dt.year == 2018) & ((tweets['created_at'].dt.week == 17) | (tweets['created_at'].dt.week == 16))].value_counts()\n",
    "#dates_values = tweets['created_at'][(tweets['created_at'].dt.year == 2018) & (tweets['created_at'].dt.week == 7 )].value_counts()\n",
    "week_s03 = tweetsS03['created_at'][(tweetsS03['extracted'] == False)].value_counts()\n",
    "week_s17 = tweets['created_at'][(tweets['extracted'] == False)].value_counts()\n",
    "\n",
    "print(week_s03.resample(\"W\").sum().values)\n",
    "print(week_s17.resample(\"W\").sum().values)\n",
    "#plt.figure(figsize=(16,6))\n",
    "#fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "\n",
    "concat_values = np.concatenate([week_s03.resample(\"W\").sum().values, week_s17.resample(\"W\").sum().values])\n",
    "concat_weeks = [\"03\"] + [\"17\"]\n",
    "print(concat_values)\n",
    "print(concat_weeks)\n",
    "\n",
    "data = {'number of tweets':concat_values, 'calendar week': concat_weeks}\n",
    "df_week_stats = pd.DataFrame(data=data)\n",
    "\n",
    "df_days_stats\n",
    "ax = sns.barplot(x=\"calendar week\", y=\"number of tweets\", data=df_week_stats)\n",
    "ax.set_title(\"Tweets collected via Filtered Stream\")\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "plt.savefig(\"streaming_stats/filterd-count-by-week.png\", transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dates_values = tweets['created_at'][(tweets['created_at'].dt.day == 23) & (tweets['extracted'] == False) & (tweets['created_at'].dt.year == 2018) & ((tweets['created_at'].dt.week == 17) | (tweets['created_at'].dt.week == 16))].value_counts()\n",
    "#dates_values = tweets['created_at'][(tweets['created_at'].dt.year == 2018) & (tweets['created_at'].dt.week == 7 )].value_counts()\n",
    "dates_values15_s03 = tweetsS03['created_at'][(tweetsS03['created_at'].dt.day == 15) & (tweetsS03['created_at'].dt.year == 2018) & (tweetsS03['created_at'].dt.month == 1) & (tweetsS03['extracted'] == False)].value_counts()\n",
    "dates_values16_s03 = tweetsS03['created_at'][(tweetsS03['created_at'].dt.day == 16) & (tweetsS03['created_at'].dt.year == 2018) & (tweetsS03['created_at'].dt.month == 1) & (tweetsS03['extracted'] == False)].value_counts()\n",
    "dates_values17_s03 = tweetsS03['created_at'][(tweetsS03['created_at'].dt.day == 17) & (tweetsS03['created_at'].dt.year == 2018) & (tweetsS03['created_at'].dt.month == 1) & (tweetsS03['extracted'] == False)].value_counts()\n",
    "dates_values18_s03 = tweetsS03['created_at'][(tweetsS03['created_at'].dt.day == 18) & (tweetsS03['created_at'].dt.year == 2018) & (tweetsS03['created_at'].dt.month == 1) & (tweetsS03['extracted'] == False)].value_counts()\n",
    "dates_values19_s03 = tweetsS03['created_at'][(tweetsS03['created_at'].dt.day == 19) & (tweetsS03['created_at'].dt.year == 2018) & (tweetsS03['created_at'].dt.month == 1) & (tweetsS03['extracted'] == False)].value_counts()\n",
    "dates_values20_s03 = tweetsS03['created_at'][(tweetsS03['created_at'].dt.day == 20) & (tweetsS03['created_at'].dt.year == 2018) & (tweetsS03['created_at'].dt.month == 1) & (tweetsS03['extracted'] == False)].value_counts()\n",
    "dates_values21_s03 = tweetsS03['created_at'][(tweetsS03['created_at'].dt.day == 21) & (tweetsS03['created_at'].dt.year == 2018) & (tweetsS03['created_at'].dt.month == 1) & (tweetsS03['extracted'] == False)].value_counts()\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.plot(dates_values15_s03.resample('h').sum().values, label=\"monday\")\n",
    "plt.plot(dates_values16_s03.resample('h').sum().values, label=\"tuesday\")\n",
    "plt.plot(dates_values17_s03.resample('h').sum().values, label=\"wednesday\")\n",
    "plt.plot(dates_values18_s03.resample('h').sum().values, label=\"thursday\")\n",
    "plt.plot(dates_values19_s03.resample('h').sum().values, label=\"friday\")\n",
    "plt.plot(dates_values20_s03.resample('h').sum().values, label=\"saturday\")\n",
    "plt.plot(dates_values21_s03.resample('h').sum().values, label=\"sunday\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Tweets collected from Filtered API by hour (calendar week 03)', fontsize=14)\n",
    "plt.ylabel('number of tweets per hour')\n",
    "plt.xlabel('hour')\n",
    "plt.axis([0, 24, 0, 1100])\n",
    "#plt.show()\n",
    "plt.savefig(\"streaming_stats/f03-by-hour.png\", transparent=True)\n",
    "#dates_values.resample('D').mean().plot(figsize=(16,6))\n",
    "#dates_values23.resample('h').sum().plot(figsize=(16,6))\n",
    "#dates_values24.resample('h').sum().plot(figsize=(16,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#dates_values = tweets['created_at'][(tweets['created_at'].dt.day == 23) & (tweets['extracted'] == False) & (tweets['created_at'].dt.year == 2018) & ((tweets['created_at'].dt.week == 17) | (tweets['created_at'].dt.week == 16))].value_counts()\n",
    "#dates_values = tweets['created_at'][(tweets['created_at'].dt.year == 2018) & (tweets['created_at'].dt.week == 7 )].value_counts()\n",
    "dates_values23_s17 = tweets['created_at'][(tweets['created_at'].dt.day == 23) & (tweets['created_at'].dt.year == 2018) & (tweets['created_at'].dt.month == 4) & (tweets['extracted'] == False)].value_counts()\n",
    "dates_values24_s17 = tweets['created_at'][(tweets['created_at'].dt.day == 24) & (tweets['created_at'].dt.year == 2018) & (tweets['created_at'].dt.month == 4) & (tweets['extracted'] == False)].value_counts()\n",
    "dates_values25_s17 = tweets['created_at'][(tweets['created_at'].dt.day == 25) & (tweets['created_at'].dt.year == 2018) & (tweets['created_at'].dt.month == 4) & (tweets['extracted'] == False)].value_counts()\n",
    "dates_values26_s17 = tweets['created_at'][(tweets['created_at'].dt.day == 26) & (tweets['created_at'].dt.year == 2018) & (tweets['created_at'].dt.month == 4) & (tweets['extracted'] == False)].value_counts()\n",
    "dates_values27_s17 = tweets['created_at'][(tweets['created_at'].dt.day == 27) & (tweets['created_at'].dt.year == 2018) & (tweets['created_at'].dt.month == 4) & (tweets['extracted'] == False)].value_counts()\n",
    "dates_values28_s17 = tweets['created_at'][(tweets['created_at'].dt.day == 28) & (tweets['created_at'].dt.year == 2018) & (tweets['created_at'].dt.month == 4) & (tweets['extracted'] == False)].value_counts()\n",
    "dates_values29_s17 = tweets['created_at'][(tweets['created_at'].dt.day == 29) & (tweets['created_at'].dt.year == 2018) & (tweets['created_at'].dt.month == 4) & (tweets['extracted'] == False)].value_counts()\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.plot(dates_values23_s17.resample('h').sum().values, label=\"monday\")\n",
    "plt.plot(dates_values24_s17.resample('h').sum().values, label=\"tuesday\")\n",
    "plt.plot(dates_values25_s17.resample('h').sum().values, label=\"wednesday\")\n",
    "plt.plot(dates_values26_s17.resample('h').sum().values, label=\"thursday\")\n",
    "plt.plot(dates_values27_s17.resample('h').sum().values, label=\"friday\")\n",
    "plt.plot(dates_values28_s17.resample('h').sum().values, label=\"saturday\")\n",
    "plt.plot(dates_values29_s17.resample('h').sum().values, label=\"sunday\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Tweets collected from Filtered API by hour (calendar week 17)', fontsize=14)\n",
    "plt.ylabel('number of tweets per hour')\n",
    "plt.xlabel('hour')\n",
    "plt.axis([0, 24, 0, 1100])\n",
    "#plt.show()\n",
    "plt.savefig(\"streaming_stats/f17-by-hour.png\", transparent=True)\n",
    "#dates_values.resample('D').mean().plot(figsize=(16,6))\n",
    "#dates_values23.resample('h').sum().plot(figsize=(16,6))\n",
    "#dates_values24.resample('h').sum().plot(figsize=(16,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dates_values23_s17.resample('h').sum().values)\n",
    "print(dates_values15_s03.resample('h').sum().values)\n",
    "print((dates_values23_s17.resample('h').sum().values + dates_values15_s03.resample('h').sum().values) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dates_values = tweets['created_at'][(tweets['created_at'].dt.day == 23) & (tweets['extracted'] == False) & (tweets['created_at'].dt.year == 2018) & ((tweets['created_at'].dt.week == 17) | (tweets['created_at'].dt.week == 16))].value_counts()\n",
    "#dates_values = tweets['created_at'][(tweets['created_at'].dt.year == 2018) & (tweets['created_at'].dt.week == 7 )].value_counts()\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.plot((dates_values23_s17.resample('h').sum().values + dates_values15_s03.resample('h').sum().values) / 2, label=\"monday\")\n",
    "plt.plot((dates_values24_s17.resample('h').sum().values + dates_values16_s03.resample('h').sum().values) / 2, label=\"tuesday\")\n",
    "plt.plot((dates_values25_s17.resample('h').sum().values + dates_values17_s03.resample('h').sum().values) / 2, label=\"wednesday\")\n",
    "plt.plot((dates_values26_s17.resample('h').sum().values + dates_values18_s03.resample('h').sum().values) / 2, label=\"thursday\")\n",
    "plt.plot((dates_values27_s17.resample('h').sum().values + dates_values19_s03.resample('h').sum().values) / 2, label=\"friday\")\n",
    "plt.plot((dates_values28_s17.resample('h').sum().values + dates_values20_s03.resample('h').sum().values) / 2, label=\"saturday\")\n",
    "plt.plot((dates_values29_s17.resample('h').sum().values + dates_values21_s03.resample('h').sum().values) / 2, label=\"sunday\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Tweets collected from Filtered API by hour (mean)', fontsize=14)\n",
    "plt.ylabel('number of tweets per hour')\n",
    "plt.xlabel('hour')\n",
    "plt.axis([0, 24, 0, 1100])\n",
    "#plt.show()\n",
    "plt.savefig(\"streaming_stats/filterd-by-hour-mean.png\", transparent=True)\n",
    "#dates_values.resample('D').mean().plot(figsize=(16,6))\n",
    "#dates_values23.resample('h').sum().plot(figsize=(16,6))\n",
    "#dates_values24.resample('h').sum().plot(figsize=(16,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dates_values = tweets['created_at'][(tweets['created_at'].dt.day == 23) & (tweets['extracted'] == False) & (tweets['created_at'].dt.year == 2018) & ((tweets['created_at'].dt.week == 17) | (tweets['created_at'].dt.week == 16))].value_counts()\n",
    "#dates_values = tweets['created_at'][(tweets['created_at'].dt.year == 2018) & (tweets['created_at'].dt.week == 7 )].value_counts()\n",
    "plt.figure(figsize=(16,6))\n",
    "date1_s = (dates_values23_s17.resample('h').sum().values + dates_values15_s03.resample('h').sum().values) / 2\n",
    "date2_s = (dates_values24_s17.resample('h').sum().values + dates_values16_s03.resample('h').sum().values) / 2\n",
    "date3_s = (dates_values25_s17.resample('h').sum().values + dates_values17_s03.resample('h').sum().values) / 2\n",
    "date4_s = (dates_values26_s17.resample('h').sum().values + dates_values18_s03.resample('h').sum().values) / 2\n",
    "date5_s = (dates_values27_s17.resample('h').sum().values + dates_values19_s03.resample('h').sum().values) / 2\n",
    "date6_s = (dates_values28_s17.resample('h').sum().values + dates_values20_s03.resample('h').sum().values) / 2\n",
    "date7_s = (dates_values29_s17.resample('h').sum().values + dates_values21_s03.resample('h').sum().values) / 2\n",
    "\n",
    "mean_over_weekdays = (date1_s + date2_s + date3_s + date4_s + date5_s + date6_s + date7_s) / 7\n",
    "\n",
    "plt.plot(mean_over_weekdays)\n",
    "plt.legend()\n",
    "plt.title('Tweets collected from Filtered API by hour (mean over week)', fontsize=14)\n",
    "plt.ylabel('number of tweets per hour')\n",
    "plt.xlabel('hour')\n",
    "plt.axis([0, 24, 0, 1100])\n",
    "\n",
    "#plt.show()\n",
    "plt.savefig(\"streaming_stats/filterd-by-hour-over-week-mean.png\", transparent=True)\n",
    "#dates_values.resample('D').mean().plot(figsize=(16,6))\n",
    "#dates_values23.resample('h').sum().plot(figsize=(16,6))\n",
    "#dates_values24.resample('h').sum().plot(figsize=(16,6))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
